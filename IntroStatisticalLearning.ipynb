{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6a727e0",
   "metadata": {},
   "source": [
    "##### What Is Statistics Learning?\n",
    "\n",
    "Statistical learning refers to a set of tools for modeling and understanding complex datasets. It is a recently developed area in statistics and blends parallel developments in computer science and machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8cd57e",
   "metadata": {},
   "source": [
    "More generally we suppose we observe a response $ Y $ and $ p $ different predictors, $ X_1, X_2, ..., X_p $. We further assume some relationship between $ Y $ and $ X = (X_1, X_2, ..., X_p) $, which is expressed in the general form $$ Y = f(X) + \\epsilon $$\n",
    "\n",
    "Here $f$ is some fixed but unknown function of $X$ and $\\epsilon$ is a random error term which is independent of $X$ and have mean zero. \n",
    "\n",
    "In essence, statistical learning refers to a set of approaches for estimating $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79ffcf9",
   "metadata": {},
   "source": [
    "#### Why estimate $f$?\n",
    "There are two main reasons we may wish to estimate $f$: prediction and inference. \n",
    "\n",
    "##### Prediction\n",
    "In many situations, a set of inputs $X$ are readily available, but the output Y cannot be easily obtained. In this setting, we predict $Y$ using $$ \\hat{Y} = \\hat{f}(X),$$ where $\\hat{f}$ represents the resulting prediction for $f$, and $\\hat{Y}$ represents the resulting prediction for $Y$. \n",
    "\n",
    "The accuracy of $\\hat{Y}$ as a prediction for $Y$ depends on two quantities: reducible error and irreducible error. In general, $\\hat{f}$ will not be a perfect estimate for $f$, and this inaccuracy will introduce some error. This error is *reduceable* because we can potentially improve the accuracy of $\\hat{f}$ by using the most appropriate statistical learning technique to estiamte $\\hat{f}$.\n",
    "\n",
    "The variability associated with $\\epsilon$, which is out of our control, will still introduce error in how well we can estimate $Y$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40d8963",
   "metadata": {},
   "source": [
    "Consider a given estimate $\\hat{f}$ and a set of predictions $X$ which yeilds the prediction $\\hat{Y} = \\hat{f}(X)$. Then the expected value of the difference between $Y$ and $\\hat{Y}$, or the expected error in our prediction, is equal to $$E(Y - \\hat{Y})^2 = E[f(X) + \\epsilon - \\hat{f}(X)]^2$$ $$= [f(X) - \\hat{f}(X)]^2 + Var(\\epsilon)$$\n",
    "\n",
    "Therefore, **our goal is to esitmate $f$ with the aim of minimizing the reducible error.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9823360d",
   "metadata": {},
   "source": [
    "##### How do we estimate $f$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a25f019",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
